{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE",
    "ExecuteTime": {
     "start_time": "2023-04-19T23:47:00.344232Z",
     "end_time": "2023-04-19T23:47:00.353065Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tensorflow 버전 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## 파트 1 - 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Training set 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 이미지 데이터를 학습하기 전에 데이터 증강을 수행하는 코드\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,    # 이미지의 픽셀값을 0과 1 사이의 값으로 조정\n",
    "                                   shear_range = 0.2,    # 이미지를 변형시키는 시어링(Shearing) 강도를 지정(회전)\n",
    "                                   zoom_range = 0.2,     # 이미지를 확대 또는 축소하는 줌(Zoom) 범위를 지정\n",
    "                                   horizontal_flip = True)   # 이미지를 좌우로 뒤집는 수평방향 뒤집기를 수행\n",
    "\n",
    "# flow_from_directory() 함수를 사용하여 디렉토리에서 이미지 데이터를 읽어와 배치(batch) 단위로 학습할 수 있도록 설정\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',   # 학습 데이터셋 디렉토리 경로를 지정\n",
    "                                                 target_size = (64, 64),   # 이미지를 지정된 크기(64x64)로 리사이즈\n",
    "                                                 batch_size = 32,          # 한 번에 학습할 배치(batch) 크기를 지정(이미지의 장수 32는 기본값)\n",
    "                                                 class_mode = 'binary')     # 이진 분류(Binary classification) 문제로 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Test set 전처리\n",
    "\n",
    "데이터 증강을 수행하지 않는 이유는 모델의 일반화 성능을 평가하기 위해서이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:42:44.114229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 각 레이어가 정확히 하나의 입력 텐서와 하나의 출력 텐서를 갖는 단일 입력 및 출력으로 간단한 모델을 만들 때 유용\n",
    "\n",
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "# Sequential 모델에 컨볼루션 레이어를 추가 계층에는 커널 크기가 3x3인 32개의 필터가 있으며 사용된 활성화 함수는 ReLU\n",
    "# 레이어의 입력 모양은 [64, 64, 3]이며 크기가 64x64이고 3개의 색상 채널(RGB)이 있는 입력 이미지\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "# cnn에 최대 풀링 계층을 추가\n",
    "# MaxPool2D 계층은 pool_size 및 strides 매개변수로 정의된 크기 창 내에서 최대값을 취하여 입력의 공간적 차원을 줄인다.\n",
    "# 이 경우 pool_size=2는 풀링 작업에 2x2 윈도우가 사용됨을 의미하고 strides=2는 윈도우가 한 번에 2 단위 이동됨을 의미\n",
    "# 이 계층의 출력은 입력의 다운샘플링된 버전이며 공간 차원은 2배로 줄었다.\n",
    "# padding의 기본값은 valid이고 2칸씩 띄었을 때 없는 숫자 칸은 무시 할 수 있고\n",
    "# padding의 same은 없는 숫자 칸은 0인 가짜 픽셀을 추가 한다.\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "# cnn에 평탄화 레이어를 추가\n",
    "# Flatten 레이어는 이전 레이어의 출력을 가져와 완전 연결 레이어로 전달할 수 있는 1차원 배열로 변환\n",
    "# 이는 일반적으로 이전 레이어에서 추출한 기능 간의 관계를 학습하고 모델링할 수 있는 네트워크에 조밀한 레이어를 추가하기 전에 수행\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 완전히 연결된 계층을 신경망에 추가\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 완전히 연결된 계층을 신경망에 추가\n",
    "# 이진 분류 문제이므로 마지막 계층의 뉴런 수는 1로 설정\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "#  'adam' 옵티마이저는 손실 함수의 기울기에 따라 네트워크 가중치를 업데이트하는 데 사용\n",
    "#  'binary_crossentropy' 손실 함수는 이진 분류 문제에 사용되며 잘못된 예측에 대해 모델에 페널티를 줌\n",
    "#  학습 및 테스트 중에 모델의 성능을 평가하는 데 사용되는 메트릭은 모델에서 수행한 올바른 예측의 백분율인 정확도\n",
    "#  compile() 함수는 모델을 컴파일하고 학습을 위해 준비하는 데 사용\n",
    "\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 25s 95ms/step - loss: 0.6780 - accuracy: 0.5740 - val_loss: 0.6103 - val_accuracy: 0.6810\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.6095 - accuracy: 0.6646 - val_loss: 0.5600 - val_accuracy: 0.7235\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.5631 - accuracy: 0.7097 - val_loss: 0.5850 - val_accuracy: 0.6950\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.5361 - accuracy: 0.7314 - val_loss: 0.5140 - val_accuracy: 0.7550\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.5103 - accuracy: 0.7509 - val_loss: 0.5257 - val_accuracy: 0.7430\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.4868 - accuracy: 0.7686 - val_loss: 0.5013 - val_accuracy: 0.7515\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.4823 - accuracy: 0.7695 - val_loss: 0.4888 - val_accuracy: 0.7730\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.4596 - accuracy: 0.7778 - val_loss: 0.5821 - val_accuracy: 0.7275\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.4504 - accuracy: 0.7859 - val_loss: 0.4644 - val_accuracy: 0.7870\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.4301 - accuracy: 0.7997 - val_loss: 0.4822 - val_accuracy: 0.7780\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.4189 - accuracy: 0.8040 - val_loss: 0.4863 - val_accuracy: 0.7830\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.4119 - accuracy: 0.8091 - val_loss: 0.4822 - val_accuracy: 0.7850\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 20s 82ms/step - loss: 0.3894 - accuracy: 0.8204 - val_loss: 0.5268 - val_accuracy: 0.7570\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.3764 - accuracy: 0.8303 - val_loss: 0.4786 - val_accuracy: 0.7825\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.3821 - accuracy: 0.8255 - val_loss: 0.4573 - val_accuracy: 0.7880\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.3595 - accuracy: 0.8340 - val_loss: 0.5024 - val_accuracy: 0.7815\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.3488 - accuracy: 0.8435 - val_loss: 0.4548 - val_accuracy: 0.8045\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.3410 - accuracy: 0.8493 - val_loss: 0.4619 - val_accuracy: 0.8065\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.3351 - accuracy: 0.8485 - val_loss: 0.4761 - val_accuracy: 0.8035\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3177 - accuracy: 0.8596 - val_loss: 0.5194 - val_accuracy: 0.7815\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.3159 - accuracy: 0.8640 - val_loss: 0.4795 - val_accuracy: 0.8050\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.3039 - accuracy: 0.8668 - val_loss: 0.4777 - val_accuracy: 0.7860\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.2978 - accuracy: 0.8698 - val_loss: 0.4676 - val_accuracy: 0.8045\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.2928 - accuracy: 0.8736 - val_loss: 0.5087 - val_accuracy: 0.7920\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.2772 - accuracy: 0.8819 - val_loss: 0.5560 - val_accuracy: 0.7815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8053f90880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_set에 지정된 훈련 데이터에서 컴파일된 CNN 모델 cnn을 훈련하고 총 25 에포크 동안 test_set에 지정된 테스트 데이터에서 검증\n",
    "# 교육 과정에서 모델은 Adam 옵티마이저를 사용하여 이진 교차 엔트로피 손실을 최소화하려고 시도하고 '정확도' 메트릭을 사용하여 모델의 정확도를 모니터링한다.\n",
    "\n",
    "# 'fit()' 메서드는 모델을 교육 데|이터에 맞추고 지정된 에포크 수 동안 테스트 데이터에서 유효성을 검사하는 데 사용\n",
    "# 각 에포크 동안 모델은 지정된 배치 크기(이 경우 32개)의 배치에 대해 학습되며 유효성 검사 정확도 및 손실은 각 에포크 후에 계산\n",
    "# 'fit()' 메서드의 출력은 각 시대의 모델 손실 및 정확도와 같은 교육 프로세스에 대한 정보를 포함하는 'History' 개체\n",
    "\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "#  훈련된 CNN 모델을 사용하여 단일 이미지에 대한 예측을 하기 위한 것\n",
    "import numpy as np\n",
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "# cnn모델에 입력할 크기로 조절 하면서 이미지를 불러온다.\n",
    "test_image = load_img('dataset/prediction/dog.jpg', target_size = (64, 64))\n",
    "# 이미지를 numpy배열로 변환\n",
    "test_image = img_to_array(test_image)\n",
    "# 배열의 차원을 cnn 모델의 입력 차원에 맞추기 위해 확장\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "# cnn 모델을 사용하여 이미지에 대한 예측 수행\n",
    "result = cnn.predict(test_image)\n",
    "# 에측 결과를 레이블로 매핑하기 위해 훈련 데이터의 클래스 인덱스 를 가져온다.\n",
    "training_set.class_indices\n",
    "# 예측 결과가 1 이상인 경우 'dog' 그렇지 않은 경우 'cat'\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
